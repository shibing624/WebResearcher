# -*- coding: utf-8 -*-
"""
@author:XuMing(xuming624@qq.com)
@description: Test-Time Scaling (TTS) Agent - Optional inference enhancement

This module implements the Test-Time Scaling technique from the WebResearcher paper.
It's an OPTIONAL inference enhancement that trades cost (3-5x tokens) for higher accuracy.

âš ï¸ COST WARNING:
- Running N parallel agents costs approximately N Ã— single-agent cost
- Synthesis step adds ~0.5x additional cost
- Total cost: ~(N + 0.5)x of single-agent baseline

ðŸ’¡ WHEN TO USE:
âœ… High-value scenarios:
   - Scientific research questions
   - Medical/legal analysis
   - Critical investment decisions
   - Complex technical problem-solving

âŒ DON'T use for:
   - Daily queries
   - Simple questions
   - Cost-sensitive applications
   - Real-time interactions

For 95% of use cases, prefer the single WebResearcherAgent (agent.py).
"""

import asyncio
from typing import Dict, List, Optional

from loguru import logger

from webresearcher.agent import WebResearcherAgent


class TestTimeScalingAgent:
    """
    Test-Time Scaling (TTS) Agent with parallel research and synthesis.
    
    This agent runs multiple IterResearch agents in parallel with different
    temperatures to encourage diverse exploration paths, then synthesizes
    their findings into a final answer.
    
    This is analogous to:
    - Self-Consistency sampling in chain-of-thought
    - Best-of-N sampling
    - Monte Carlo Tree Search (MCTS) in game playing
    
    Args:
        llm_config: LLM configuration dict
        function_list: List of tool names to enable
    
    Example:
        >>> agent = TestTimeScalingAgent(llm_config, ["search", "PythonInterpreter"])
        >>> result = await agent.run("Complex question", num_parallel_agents=3)
        >>> print(result["final_synthesized_answer"])
    """

    def __init__(self, llm_config: Dict, function_list: List[str]):
        self.llm_config = llm_config
        self.function_list = function_list

    def estimate_cost(self, num_parallel_agents: int = 3) -> str:
        """
        Estimate the cost multiplier of using TTS.
        
        Args:
            num_parallel_agents: Number of parallel research agents
            
        Returns:
            Cost estimation message
        """
        total_cost = num_parallel_agents + 0.5
        return (
            f"âš ï¸  TTS Cost Estimation:\n"
            f"   â€¢ Parallel research: {num_parallel_agents} agents Ã— base cost\n"
            f"   â€¢ Synthesis: ~0.5Ã— base cost\n"
            f"   â€¢ Total: ~{total_cost:.1f}Ã— of single-agent baseline\n"
            f"   â€¢ Use only for high-value scenarios!"
        )

    async def run_parallel_research(
        self, 
        question: str, 
        num_parallel_agents: int = 3
    ) -> List[Dict]:
        """
        Phase 1: Parallel Research with diverse exploration.
        
        Runs multiple IterResearch agents in parallel, each with different
        temperature settings to encourage diverse reasoning paths.
        
        Args:
            question: Research question
            num_parallel_agents: Number of parallel agents (default: 3)
            
        Returns:
            List of research results from each agent
        """
        logger.info(f"ðŸš€ Starting Parallel Research Phase ({num_parallel_agents} agents)")
        logger.warning(self.estimate_cost(num_parallel_agents))

        tasks = []
        for i in range(num_parallel_agents):
            # Create a copy of config for each agent
            agent_llm_config = self.llm_config.copy()
            agent_llm_config["generate_cfg"] = agent_llm_config.get("generate_cfg", {}).copy()
            
            # Adjust temperature for diversity (e.g., 0.5, 0.7, 0.9)
            base_temp = agent_llm_config["generate_cfg"].get("temperature", 0.6)
            agent_llm_config["generate_cfg"]["temperature"] = base_temp + (i * 0.2)
            
            # Create agent instance
            agent = WebResearcherAgent(
                llm_config=agent_llm_config,
                function_list=self.function_list,
            )
            
            # Add to parallel tasks
            tasks.append(agent.run(question))
            logger.info(
                f"   â€¢ Agent {i+1}: temperature={agent_llm_config['generate_cfg']['temperature']:.2f}"
            )

        # Execute all agents in parallel
        parallel_results = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results
        valid_results = []
        for i, res in enumerate(parallel_results):
            if isinstance(res, Exception):
                logger.error(f"âŒ Agent {i+1} failed with exception: {res}")
            elif isinstance(res, dict):
                status = res.get("termination", "unknown")
                if status == "answer":
                    logger.info(f"âœ… Agent {i+1} succeeded (status: {status})")
                else:
                    logger.warning(f"âš ï¸  Agent {i+1} finished with status: {status}")
                valid_results.append(res)
            else:
                logger.error(f"âŒ Agent {i+1} returned unexpected type: {type(res)}")

        logger.info(f"âœ… Parallel Research Complete: {len(valid_results)}/{num_parallel_agents} agents succeeded")
        return valid_results

    async def run_synthesis(
        self, 
        question: str, 
        parallel_results: List[Dict]
    ) -> Dict:
        """
        Phase 2: Integrative Synthesis.
        
        Synthesizes findings from multiple parallel research agents into
        a single, high-quality final answer.
        
        Args:
            question: Original research question
            parallel_results: Results from parallel research phase
            
        Returns:
            Dict with final_answer and synthesis_reports
        """
        logger.info("ðŸ”„ Starting Integrative Synthesis Phase")

        if not parallel_results:
            logger.error("No valid results from parallel research. Cannot synthesize.")
            return {
                "final_answer": "Synthesis failed: No research data available.",
                "synthesis_reports": []
            }

        # Build synthesis prompt
        synthesis_prompt_content = f"ã€åŽŸå§‹ç ”ç©¶é—®é¢˜ã€‘\n{question}\n\n"
        synthesis_prompt_content += "ã€æ¥è‡ªå¤šä¸ªå¹¶è¡Œç ”ç©¶å‘˜çš„æŠ¥å‘Šå’Œç­”æ¡ˆã€‘\n"

        reports_for_log = []
        for i, res in enumerate(parallel_results):
            synthesis_prompt_content += f"\n--- ç ”ç©¶å‘˜ {i + 1} (çŠ¶æ€: {res.get('termination', 'unknown')}) ---\n"
            synthesis_prompt_content += f"ã€ç ”ç©¶å‘˜ {i + 1} çš„ç­”æ¡ˆã€‘\n{res.get('prediction', 'N/A')}\n"
            synthesis_prompt_content += f"ã€ç ”ç©¶å‘˜ {i + 1} çš„æœ€ç»ˆæŠ¥å‘Šã€‘\n{res.get('report', 'N/A')}\n"
            
            reports_for_log.append({
                "agent": i + 1,
                "answer": res.get('prediction'),
                "report": res.get('report'),
                "termination": res.get('termination')
            })

        synthesis_messages = [
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„ã€é¦–å¸­ç ”ç©¶å‘˜ã€‘ï¼Œè´Ÿè´£ç»¼åˆå¤šä¸ªç ”ç©¶å‘˜çš„å‘çŽ°ã€‚\n"
                    "ä½ çš„ä»»åŠ¡æ˜¯å®¡æŸ¥æ¥è‡ªå¤šä¸ªå¹¶è¡Œç ”ç©¶å‘˜çš„æŠ¥å‘Šå’Œç­”æ¡ˆï¼Œç„¶åŽç»¼åˆæ‰€æœ‰ä¿¡æ¯ï¼Œ"
                    "å¾—å‡ºä¸€ä¸ªå”¯ä¸€çš„ã€æœ€å‡†ç¡®ã€æœ€å…¨é¢çš„æœ€ç»ˆç­”æ¡ˆã€‚\n\n"
                    "å·¥ä½œæµç¨‹ï¼š\n"
                    "1. äº¤å‰éªŒè¯ï¼šæ¯”è¾ƒä¸åŒæŠ¥å‘Šä¸­çš„äº‹å®žå’Œç»“è®ºï¼Œè¯†åˆ«ä¸€è‡´æ€§å’Œå·®å¼‚ã€‚\n"
                    "2. è§£å†³å†²çªï¼šå¦‚æžœæŠ¥å‘Šå†²çªï¼Œè¯·æ ¹æ®è¯æ®è´¨é‡å’Œé€»è¾‘ä¸¥å¯†æ€§åšå‡ºæœ€ä½³åˆ¤æ–­ã€‚\n"
                    "3. ç»¼åˆæç‚¼ï¼šä¸è¦åªé€‰æ‹©ä¸€ä¸ªç­”æ¡ˆï¼Œè¦æ•´åˆæ‰€æœ‰æŠ¥å‘Šä¸­çš„æœ‰æ•ˆä¿¡æ¯ï¼Œå½¢æˆä¸€ä¸ªæ›´ä¼˜çš„ç­”æ¡ˆã€‚\n"
                    "4. è´¨é‡ä¼˜å…ˆï¼šä¼˜å…ˆé‡‡çº³é€»è¾‘æ¸…æ™°ã€è¯æ®å……åˆ†çš„ç»“è®ºã€‚\n\n"
                    "è¾“å‡ºè¦æ±‚ï¼š\n"
                    "- åªè¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œä¸è¦è®¨è®ºä½ çš„ç»¼åˆè¿‡ç¨‹\n"
                    "- ç­”æ¡ˆè¦å‡†ç¡®ã€ç®€æ´ã€æœ‰æ®å¯æŸ¥"
                )
            },
            {"role": "user", "content": synthesis_prompt_content}
        ]

        # Create synthesis agent (low temperature for stability)
        synthesis_llm_config = self.llm_config.copy()
        synthesis_llm_config["generate_cfg"] = synthesis_llm_config.get("generate_cfg", {}).copy()
        synthesis_llm_config["generate_cfg"]["temperature"] = 0.2

        synthesis_agent = WebResearcherAgent(
            llm_config=synthesis_llm_config,
            function_list=[],  # Synthesis agent doesn't need tools
        )

        # Call LLM for synthesis
        logger.info("ðŸ¤– Calling synthesis LLM...")
        final_answer_raw = await synthesis_agent.call_server(
            synthesis_messages,
            stop_sequences=[]  # No tool stop tokens needed
        )

        logger.info("âœ… Synthesis Complete")
        return {
            "final_answer": final_answer_raw.strip(),
            "synthesis_reports": reports_for_log
        }

    async def run(
        self,
        question: str,
        ground_truth: Optional[str] = None,
        num_parallel_agents: int = 3
    ) -> Dict:
        """
        Main entry point for Test-Time Scaling.
        
        Args:
            question: Research question
            ground_truth: Ground truth answer (optional, for evaluation)
            num_parallel_agents: Number of parallel agents (default: 3)
            
        Returns:
            Dict containing:
                - question: Original question
                - ground_truth: Ground truth (if provided)
                - final_synthesized_answer: Final synthesized answer
                - parallel_runs: All parallel agent results
                - synthesis_inputs: Reports used for synthesis
        """
        logger.info(f"ðŸŽ¯ Starting Test-Time Scaling for: {question[:100]}...")
        
        # Phase 1: Parallel Research
        parallel_results = await self.run_parallel_research(question, num_parallel_agents)

        # Phase 2: Integrative Synthesis
        synthesis_result = await self.run_synthesis(question, parallel_results)

        return {
            "question": question,
            "ground_truth": ground_truth,
            "final_synthesized_answer": synthesis_result["final_answer"],
            "parallel_runs": parallel_results,
            "synthesis_inputs": synthesis_result["synthesis_reports"]
        }

